"""
M√≥dulo Analyzer Principal
Orquesta todo el an√°lisis de InterOrdra
"""

from backend.embeddings import SemanticEmbedder
from backend.clustering import ConceptClusterer
from backend.gap_detector import GapDetector
import numpy as np
from typing import Dict

class InterOrdraAnalyzer:
    """
    Analizador principal de InterOrdra.
    
    Coordina:
    - Generaci√≥n de embeddings
    - Clustering de conceptos
    - Detecci√≥n de gaps
    - Visualizaci√≥n de datos
    """
    
    def __init__(self, language: str = 'es', threshold: float = 0.5):
        """
        Inicializa el analyzer.
        
        Args:
            language: 'es' o 'en'
            threshold: Umbral de similaridad para considerar acoplamiento
        """
        print("üîÑ Inicializando InterOrdra...")
        
        self.embedder = SemanticEmbedder(language=language)
        self.clusterer = ConceptClusterer(eps=0.5, min_samples=2)
        self.gap_detector = GapDetector(threshold=threshold, language=language)
        
        print("‚úÖ InterOrdra listo para analizar\n")
    
    def analyze(self, text_a: str, text_b: str, label_a: str = "Texto A", label_b: str = "Texto B") -> Dict:
        """
        Pipeline completo de an√°lisis.
        
        Args:
            text_a: Primer texto (ej: documentaci√≥n t√©cnica)
            text_b: Segundo texto (ej: pregunta de usuario)
            label_a: Etiqueta descriptiva para texto A
            label_b: Etiqueta descriptiva para texto B
        
        Returns:
            Dict con todos los resultados del an√°lisis
        """
        print(f"üìù Analizando: '{label_a}' vs '{label_b}'")
        print("-" * 60)
        
        # 1. GENERAR EMBEDDINGS
        print("1Ô∏è‚É£  Generando embeddings sem√°nticos...")
        data_a = self.embedder.embed_text(text_a)
        data_b = self.embedder.embed_text(text_b)
        print(f"   ‚Ä¢ {label_a}: {data_a['num_sentences']} oraciones")
        print(f"   ‚Ä¢ {label_b}: {data_b['num_sentences']} oraciones")
        
        # 2. DETECTAR CLUSTERS
        print("\n2Ô∏è‚É£  Identificando clusters conceptuales...")
        clusters_a = self.clusterer.find_clusters(
            data_a['embeddings'], 
            data_a['sentences']
        )
        clusters_b = self.clusterer.find_clusters(
            data_b['embeddings'], 
            data_b['sentences']
        )
        print(f"   ‚Ä¢ {label_a}: {len([c for c in clusters_a if c != -1])} clusters")
        print(f"   ‚Ä¢ {label_b}: {len([c for c in clusters_b if c != -1])} clusters")
        
        # 3. DETECTAR GAPS
        print("\n3Ô∏è‚É£  Detectando desacoplamientos...")
        gaps = self.gap_detector.detect_gaps(data_a, data_b)
        print(f"   ‚Ä¢ Similaridad global: {gaps['global_similarity']:.1%}")
        print(f"   ‚Ä¢ Estado: {'‚úÖ ACOPLADO' if gaps['is_coupled'] else '‚ùå DESACOPLADO'}")
        
        # 4. PREPARAR VISUALIZACI√ìN
        print("\n4Ô∏è‚É£  Preparando datos de visualizaci√≥n...")
        all_embeddings = np.vstack([
            data_a['embeddings'],
            data_b['embeddings']
        ])
        coords_3d = self.clusterer.reduce_dimensions(all_embeddings, n_components=3)
        
        # Separar coordenadas
        split_point = len(data_a['sentences'])
        coords_a = coords_3d[:split_point]
        coords_b = coords_3d[split_point:]
        
        # 5. COMPILAR RESULTADOS
        results = {
            'metadata': {
                'label_a': label_a,
                'label_b': label_b,
                'threshold': self.gap_detector.threshold
            },
            'text_a': {
                'sentences': data_a['sentences'],
                'num_sentences': data_a['num_sentences'],
                'clusters': clusters_a,
                'num_clusters': len([c for c in clusters_a if c != -1]),
                'coords_3d': coords_a.tolist()
            },
            'text_b': {
                'sentences': data_b['sentences'],
                'num_sentences': data_b['num_sentences'],
                'clusters': clusters_b,
                'num_clusters': len([c for c in clusters_b if c != -1]),
                'coords_3d': coords_b.tolist()
            },
            'gaps': gaps,
            'summary': self.gap_detector.generate_summary(gaps)
        }
        
        print("\n‚úÖ An√°lisis completado\n")
        
        return results
    
    def quick_analysis(self, text_a: str, text_b: str) -> None:
        """
        An√°lisis r√°pido con output formateado a consola.
        
        Args:
            text_a, text_b: Textos a comparar
        """
        results = self.analyze(text_a, text_b)
        
        # Mostrar resumen
        print(results['summary'])
        
        # Mostrar algunos gaps espec√≠ficos
        if results['gaps']['gaps']['text_a_orphans']:
            print("üî∏ Ejemplos de conceptos hu√©rfanos en Texto A:")
            for orphan in results['gaps']['gaps']['text_a_orphans'][:2]:
                print(f"   ‚Ä¢ \"{orphan['sentence']}\"")
                print(f"     (mejor match: {orphan['best_match_similarity']:.1%})")
        
        if results['gaps']['gaps']['text_b_orphans']:
            print("\nüî∏ Ejemplos de conceptos hu√©rfanos en Texto B:")
            for orphan in results['gaps']['gaps']['text_b_orphans'][:2]:
                print(f"   ‚Ä¢ \"{orphan['sentence']}\"")
                print(f"     (mejor match: {orphan['best_match_similarity']:.1%})")
        
        # Mostrar vocabulario √∫nico
        vocab = results['gaps']['vocabulary_analysis']
        if vocab['unique_to_a']:
            print(f"\nüìö Vocabulario √∫nico en A: {', '.join(vocab['unique_to_a'][:5])}")
        if vocab['unique_to_b']:
            print(f"üìö Vocabulario √∫nico en B: {', '.join(vocab['unique_to_b'][:5])}")


def test_analyzer():
    """Test completo de InterOrdra"""
    print("\n" + "="*60)
    print("üß™ PROBANDO INTERORDRA - AN√ÅLISIS COMPLETO")
    print("="*60 + "\n")
    
    # Crear analyzer
    analyzer = InterOrdraAnalyzer(language='es', threshold=0.5)
    
    # Textos de ejemplo: documentaci√≥n t√©cnica vs pregunta de usuario
    texto_tecnico = """
    El sistema utiliza embeddings vectoriales de alta dimensi√≥n para representar 
    informaci√≥n sem√°ntica. Los transformers pre-entrenados generan representaciones 
    contextuales mediante mecanismos de atenci√≥n multi-cabeza. La arquitectura 
    implementa capas de normalizaci√≥n y conexiones residuales para estabilidad.
    """
    
    pregunta_usuario = """
    ¬øC√≥mo funciona esto? No entiendo qu√© significa que use vectores.
    ¬øQu√© es un transformer? ¬øPor qu√© necesito saber de matem√°ticas avanzadas?
    Solo quiero que mi aplicaci√≥n entienda texto en espa√±ol.
    """
    
    # Analizar
    analyzer.quick_analysis(
        texto_tecnico, 
        pregunta_usuario
    )
    
    print("\n" + "="*60)
    print("‚úÖ TEST COMPLETADO")
    print("="*60 + "\n")


if __name__ == "__main__":
    test_analyzer()