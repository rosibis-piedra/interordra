"""
M√≥dulo de Clustering Conceptual
Agrupa oraciones sem√°nticamente similares en clusters
"""

from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
import numpy as np
from typing import Dict, List, Optional

class ConceptClusterer:
    """
    Identifica clusters de conceptos relacionados en textos.
    
    Usa DBSCAN (Density-Based Spatial Clustering) que:
    - No requiere especificar n√∫mero de clusters anticipadamente
    - Detecta clusters de forma arbitraria
    - Identifica "ruido" (conceptos aislados)
    """
    
    def __init__(self, eps: float = 0.5, min_samples: int = 2):
        """
        Inicializa el clusterer.
        
        Args:
            eps: Distancia m√°xima para considerar dos puntos vecinos
                - Menor eps = clusters m√°s estrictos/peque√±os
                - Mayor eps = clusters m√°s permisivos/grandes
            min_samples: M√≠nimo de puntos para formar un cluster
        """
        self.eps = eps
        self.min_samples = min_samples
    
    def find_clusters(self, embeddings: np.ndarray, sentences: List[str]) -> Dict:
        """
        Agrupa oraciones sem√°nticamente similares.
        
        Args:
            embeddings: Array numpy con vectores sem√°nticos (shape: [n_sentences, embed_dim])
            sentences: Lista de oraciones originales
        
        Returns:
            Dict con informaci√≥n de clusters:
                {
                    cluster_id: {
                        'sentences': [...],
                        'centroid': vector_promedio,
                        'size': n√∫mero_de_oraciones
                    }
                }
        """
        if len(embeddings) == 0:
            return {}
        
        # Aplicar DBSCAN con distancia coseno
        clustering = DBSCAN(
            eps=self.eps,
            min_samples=self.min_samples,
            metric='cosine'
        ).fit(embeddings)
        
        labels = clustering.labels_
        
        # Organizar resultados por cluster
        clusters = {}
        noise_items = []
        
        for idx, label in enumerate(labels):
            # Label -1 = ruido (no pertenece a ning√∫n cluster)
            if label == -1:
                noise_items.append({
                    'sentence': sentences[idx],
                    'embedding': embeddings[idx]
                })
                continue
            
            # Crear cluster si no existe
            if label not in clusters:
                clusters[label] = []
            
            clusters[label].append({
                'sentence': sentences[idx],
                'embedding': embeddings[idx]
            })
        
        # Calcular centroides y metadata para cada cluster
        processed_clusters = {}
        for label, items in clusters.items():
            # Centroide = promedio de todos los embeddings del cluster
            embeddings_array = np.array([item['embedding'] for item in items])
            centroid = np.mean(embeddings_array, axis=0)
            
            processed_clusters[label] = {
                'sentences': [item['sentence'] for item in items],
                'centroid': centroid,
                'size': len(items),
                'density': self._calculate_density(embeddings_array)
            }
        
        # Agregar items de ruido como pseudo-cluster
        if noise_items:
            processed_clusters[-1] = {
                'sentences': [item['sentence'] for item in noise_items],
                'centroid': None,
                'size': len(noise_items),
                'density': 0.0,
                'is_noise': True
            }
        
        return processed_clusters
    
    def _calculate_density(self, embeddings: np.ndarray) -> float:
        """
        Calcula densidad del cluster (qu√© tan compactos est√°n los puntos).
        
        Args:
            embeddings: Vectores del cluster
        
        Returns:
            Densidad promedio (0 = muy disperso, 1 = muy compacto)
        """
        if len(embeddings) <= 1:
            return 1.0
        
        # Calcular distancia promedio al centroide
        centroid = np.mean(embeddings, axis=0)
        distances = np.linalg.norm(embeddings - centroid, axis=1)
        avg_distance = np.mean(distances)
        
        # Normalizar: menor distancia = mayor densidad
        density = 1.0 / (1.0 + avg_distance)
        
        return float(density)
    
    def reduce_dimensions(self, embeddings: np.ndarray, n_components: int = 3) -> np.ndarray:
        """
        Reduce dimensionalidad para visualizaci√≥n.
        
        Embeddings t√≠picamente tienen 384 o 768 dimensiones.
        Los reducimos a 2D o 3D para graficar.
        
        Args:
            embeddings: Vectores de alta dimensi√≥n
            n_components: Dimensiones finales (2 o 3)
        
        Returns:
            Array con vectores reducidos
        """
        if len(embeddings) < n_components:
            # Si hay menos puntos que componentes, usar PCA con menos componentes
            n_components = min(n_components, len(embeddings))
        
        pca = PCA(n_components=n_components)
        reduced = pca.fit_transform(embeddings)
        
        return reduced
    
    def get_cluster_summary(self, clusters: Dict) -> str:
        """
        Genera resumen legible de los clusters encontrados.
        
        Args:
            clusters: Output de find_clusters()
        
        Returns:
            String con resumen
        """
        summary_lines = ["\nüìä Resumen de Clusters:\n"]
        
        for cluster_id, cluster_info in sorted(clusters.items()):
            if cluster_id == -1:
                summary_lines.append(f"üî∏ Conceptos aislados ({cluster_info['size']}):")
            else:
                summary_lines.append(
                    f"üîπ Cluster {cluster_id} ({cluster_info['size']} oraciones, "
                    f"densidad: {cluster_info['density']:.2f}):"
                )
            
            # Mostrar primeras 3 oraciones del cluster
            for i, sentence in enumerate(cluster_info['sentences'][:3]):
                summary_lines.append(f"   {i+1}. {sentence}")
            
            if cluster_info['size'] > 3:
                summary_lines.append(f"   ... y {cluster_info['size'] - 3} m√°s")
            
            summary_lines.append("")
        
        return "\n".join(summary_lines)


# Funci√≥n de testing
def test_clusterer():
    """Prueba r√°pida del clusterer"""
    from embeddings import SemanticEmbedder
    
    print("\nüß™ Probando ConceptClusterer...\n")
    
    # Crear embedder
    embedder = SemanticEmbedder(language='es')
    
    # Texto de ejemplo con varios conceptos
    texto = """
    El lenguaje es una vibraci√≥n entre sistemas.
    Los sistemas complejos se comunican mediante patrones.
    La inteligencia artificial procesa embeddings vectoriales.
    Las redes neuronales aprenden de datos.
    La m√∫sica es una forma de comunicaci√≥n no verbal.
    El canto transmite emociones profundas.
    """
    
    # Generar embeddings
    result = embedder.embed_text(texto)
    
    # Crear clusterer y buscar grupos
    clusterer = ConceptClusterer(eps=0.4, min_samples=2)
    clusters = clusterer.find_clusters(result['embeddings'], result['sentences'])
    
    # Mostrar resumen
    print(clusterer.get_cluster_summary(clusters))
    
    print("‚úÖ Test completado!")


if __name__ == "__main__":
    test_clusterer()